<!DOCTYPE html>
<html lang="en">
<head>
	<title>Continuous Integration - Performance Testing</title>

	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<link rel="stylesheet" href="../css/site-custom.css">
	<link rel="stylesheet" href="../css/pages/index.css">

	<link rel="icon" href="../favicon.png" type="image/png">
</head>
<body>

<nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
	<a class="navbar-brand pr-5" href="../index.html"><img class="logo" src="../images/logo_h.png"></img></a>

	<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navMain" aria-controls="navMain" aria-expanded="false" aria-label="Toggle navigation">
		<span class="navbar-toggler-icon"></span>
	</button>

	<div class="collapse navbar-collapse" id="navMain">
		<ul class="navbar-nav mr-auto">
			<li class="nav-item">
				<a class="nav-link" href="https://coggle.it/diagram/Xa8LPGI8UcxYqa_G/t/my-team-is-it-enabled-from-an-modern-perspective-red_cross/79ff70c4f2d83ea65ed42592541d7fa8494ab92bb690b77a269cea1ee0669153" target="_new">
					<span class="fa fa-share-alt"></span>
					Outcomes Map
				</a>
            </li>
			<li class="nav-item">
				<a class="nav-link" href="../index.html">
					<span class="fa fa-file-text-o"></span>
					Outcome Profiles
				</a>
			</li>
		</ul>
	</div>
</nav>

<main role="main" class="container-fluid">
<div class="box-main">
	<div class="container-fluid">
		<div class="row">
			<div class="col-9">
				<h1>Continuous Integration - Performance Testing</h1>
				<div class="v-spacer"></div>
				<ul class="nav nav-tabs" id="cpTabMain" role="tablist">
					<li class="nav-item">
						<a class="nav-link active" id="tab01" data-toggle="tab" href="#tbc01" role="tab" aria-controls="ctl01" aria-selected="true">
							Overview
						</a>
					</li>
					<li class="nav-item">
						<a class="nav-link" id="tab02" data-toggle="tab" href="#tbc02" role="tab" aria-controls="ctl02" aria-selected="false">
							Value
						</a>
					</li>
					<li class="nav-item">
						<a class="nav-link" id="tab03" data-toggle="tab" href="#tbc03" role="tab" aria-controls="ctl03" aria-selected="false">
							Best Practices
						</a>
					</li>
					<li class="nav-item">
						<a class="nav-link" id="tab04" data-toggle="tab" href="#tbc04" role="tab" aria-controls="ctl04" aria-selected="false">
							References
						</a>
					</li>
				</ul>
				<div class="tab-content" id="cpTabMainContent">
					<div class="tab-pane fade show active" id="tbc01" role="tabpanel" aria-labelledby="tab01">
						<div class="container-fluid">
							<dl class="row">
								<dt class="col-2">
									Definition
								</dt>
								<dd class="col-10">
									Performance Testing is a type of testing which ensures that a system will perform well under
									its expected workload. It serves to investigate, measure and address any system bottlenecks related to
									its speed (response time), scalability and stability.
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Desired State
								</dt>
								<dd class="col-10">
									Performance testing practices should be adopted by all dev teams and for all solutions, including custom and COTS.
									The execution of performance tests should be part of the build pipeline and be as fully automated.
								</dd>
							</dl>
							<hr	class="thin_90">
							<dl class="row">
								<dt class="col-2">
									Pathway Status
								</dt>
								<dd class="col-10">
									<span class="badge badge-warning" data-toggle="tooltip" data-placement="top" title="The path to realizing the outcome is partially clear; there are some impediments remaining. Realization may be limited to certain platforms, environments, solution classes, etc…">
										Partially Cleared
									</span>
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Status Notes
								</dt>
								<dd class="col-10">
									The departmental tool selection needs to be expanded and supported.
								</dd>
							</dl>
							<hr class="thin_90">
							<dl class="row">
								<dt class="col-2">
									Realization Status
								</dt>
								<dd class="col-10">
									<span class="badge badge-danger" data-toggle="tooltip" data-placement="top" title="Few teams have implemented/realized the outcome.">
										Low
									</span>
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Status Notes
								</dt>
								<dd class="col-10">
									<ul class="padding-none">
										<li>
											Dev team culture needs to be addressed.
										</li>
										<li>
											A standard set of guidance material should be made available.
										</li>
										<li>
											Reference implementations for the various programming languages and platforms should be made available.
										</li>
									</ul>
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Current SA Actions
								</dt>
								<dd class="col-10">
									<ul class="padding-none">
										<li>
											Support the introduction of performance testing to solution teams.
											<ul>
												<li>
													Will be working to raise awareness via DevCop and meeting with individual teams.
												</li>
												<li>
													Working on a standard set of guidance materials.
												</li>
											</ul>
										</li>
										<li>
											Management engagement to help promote the adoption within the teams.
										</li>
									</ul>
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Platforms
								</dt>
								<dd class="col-10">
									On-Premise, Cloud
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Environments
								</dt>
								<dd class="col-10">
									Non-PROD
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Solution Classes
								</dt>
								<dd class="col-10">
									All
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Programming Languages
								</dt>
								<dd class="col-10">
									All
								</dd>
							</dl>
						</div>
					</div>
					<div class="tab-pane fade" id="tbc02" role="tabpanel" aria-labelledby="tab02">
						<ul>
							<li>
								Helps determine if a system meets speed, scalability and stability requirements during various situations.
								<ul>
									<li>
										Users expect a system to respond as quickly as possible and when they don’t, satisfaction decreases.
									</li>
									<li>
										Scalability is extremely important if you want more users to interact with the system.
									</li>
									<li>
										A system is expected to work at all times, when downtime occurs, users can lose confidence.
									</li>
								</ul>
							</li>
							<li>
								Helps uncover what system bottlenecks need to be improved before the product goes to production.
							</li>
							<li>
								Helps ensure that a system can run for a long period without deviations.
							</li>
							<li>
								Systems sent to production with good performance metrics are more likely to successfully meet their expected goals.
							</li>
						</ul>
					</div>
					<div class="tab-pane fade" id="tbc03" role="tabpanel" aria-labelledby="tab03">
						<dl>
							<dt>
								Consider the various types of performance testing
							</dt>
							<dd>
								<ul class="style-none padding-some">
									<li>
										<em>Load Testing</em>
										<br	/>
										Load testing checks the application's ability to perform under anticipated user loads. 
										The objective is to identify performance bottlenecks before the software application goes live.
									</li>
									<li>
										<em>Stress Testing</em>
										<br />
										Stress testing is meant to measure performance outside of the parameters of normal operating conditions.
										The solution is given more users or transactions that can be handled, the goal being to identify the solution's stability and breaking points.
									</li>
									<li>
										<em>Endurance Testing</em>
										<br />
										Endurance testing, also known as soak testing, is an evaluation of how software performs with a normal workload over an extended amount of time.
										The goal of endurance testing is to check for system problems such as memory leaks and hardware failures.
									</li>
									<li>
										<em>Spike Testing</em>
										<br />
										Spike testing verifies the solution's reaction when workloads are substantially increased quickly and repeatedly. 
										The workload is pushed beyond normal expectations for short amounts of time.
									</li>
									<li>
										<em>Volume Testing</em>
										<br />
										The purpose of volume testing is to verify how a solution performs when it is populated with large, projected amounts of data.
									</li>
									<li>
										<em>Scalability Testing</em>
										<br />
										Scalability testing is used to determine how effectively a solution handles increasing workloads. 
										This can be determined by gradually adding to the user load or data volume while monitoring system performance. 
										Alternatively the workload may be fixed, while resources such as CPUs and memory are varied.
										This type of testing can also help plan and establish capacity additions to your environment.
									</li>
								</ul>
							</dd>
						</dl>
						<dl>
							<dt>
								Understand the most common problems observed
							</dt>
							<dd>
								<ul class="style-none padding-some">
									<li>
										<em>Bottlenecking</em>
										<br />
										This occurs when data flow is interrupted or halted because there is not enough capacity to handle the workload.									</li>
									<li>
										<em>Poor scalability</em>
										<br />
										If software cannot handle the desired number of concurrent tasks, results could be delayed, errors could increase, or other unexpected behavior could occur.
									</li>
									<li>
										<em>Software configuration issues</em>
										<br />
										Often settings are not set at a sufficient level to handle the workload.
									</li>
									<li>
										<em>Insufficient hardware resources</em>
										<br />
										Performance testing may reveal physical memory constraints or low-performing CPUs.
									</li>
								</ul>
							</dd>
						</dl>
						<dl>
							<dt>
								Determine the important metrics for your solution
							</dt>
							<dd>
								<ul class="style-none padding-some">
									<li>
										<em>Average response time</em>
										<br />
										Average response time is a good metric to start gathering data on in your performance tests. Tracking this allows you
										see how your app fluctuates with more or less load. It gives you an idea of the average user experience, and it provides
										insight on regressions if something changes.
									</li>
									<li>
										<em>Peak response time</em>
										<br />
										Peak response time allows you to see the performance of slowest requests, generally by taking the 90th percentile of all
										response times. This creates a different view than an overall average. With peak response times, you can find more
										specific queries that may be problematic and know what the worst users are experiencing.
										Having a view into the slowest queries can make it easier to track down specific operations with latency, whereas the
										average response time is more general, and only gives you an idea of the entire system.
									</li>
									<li>
										<em>Error rates</em>
										<br />
										If errors occur, you'll want to know about them. Simply measuring a ratio of total requests by failed requests -- and
										failed requests can be anything that match a criteria you define -- gives you insight on what components start failing
										under a given load.
									</li>
									<li>
										<em>CPU utilization</em>
										<br />
										It's expected that CPU utilization will go up with more users, and go down with less, but do you know what happens when
										your CPU hits 100%? Being able to act preventatively under these conditions can help you build in redudancy and auto-scaling 
										systems to keep your app available.
									</li>
									<li>
										<em>Memory utilization</em>
										<br />
										Measuring memory usage during performance tests can capture the amount of memory used by the server when processing
										requests during different scenarios, like heavy load. This metric in particular is a good way to find memory leaks in
										your application, and it some cases can be coorelated with peak response times to determine slow database queries.
									</li>
								</ul>
							</dd>
						</dl>
						<dl>
							<dt>
								Test performance constantly and continuously
							</dt>
							<dd>
								<ul>
									<li>
										Performance testing should be done as early and frequently in the process as as possible.
									</li>
									<li>
										Performance testing isn’t just for completed projects; there is value in testing individual units or modules.
									</li>
									<li>
										Conduct multiple performance tests to ensure consistent findings and determine metric averages.
									</li>
									<li>
										Applications often involve multiple systems such as databases, servers, and services; test the individual units separately as well as together.
									</li>
								</ul>
							</dd>
						</dl>
						<dl>
							<dt>
								Run tests in a production-like environment
							</dt>
							<dd>
								In order to have meaningful results, it's important to ensure that the testing environment matches the production environment hardware
								components, operating system and settings, applications used on the system, and databases. Even a slightly different amount of memory or 
								CPU speed can skew results enough that they won't reflect how the production environment will behave.
							</dd>
						</dl>
						<dl>
							<dt>
								Scale your load testing
							</dt>
							<dd>
								Running a series of tests at full load can reveal an overwhelming number of performance issues, making it hard to focus on individual solutions.
								Starting with a lower load and scaling up incrementally produces results that are easier and more efficient to troubleshoot.
							</dd>
						</dl>
						<dl>
							<dt>
								Other things to consider
							</dt>
							<dd>
								<ul>
									<li>
										Involve developers, IT and testers in creating a performance testing environment.
									</li>
									<li>
										Remember to consider how the observed results will affect the solution users, not just environment hardware.
									</li>
									<li>
										Develop test scripts that takes into account as many user activities as possible.
									</li>
									<li>
										Baseline measurements provide a starting point for determining success or failure.
									</li>
									<li>
										Performance tests are best conducted in test environments that are as close to the production systems as possible.
									</li>
									<li>
										No performance testing tool will do everything needed; research performance testing tools for the right fit.
									</li>
									<li>
										Other than just calculating averages, there also value in tracking outliers; those extreme measurements could reveal possible points of failure.
									</li>
									<li>
										Consider the audience when preparing reports that share performance testing findings; mention any solution changes in reports.
									</li>
								</ul>
							</dd>
						</dl>
					</div>
					<div class="tab-pane fade" id="tbc04" role="tabpanel" aria-labelledby="tab04">
						<div class="container-fluid">
							<dl class="row">
								<dt class="col-2">
									Tool Examples
								</dt>
								<dd class="col-10">
									<div>
										<a href="https://jmeter.apache.org/" target="_blank">
											Apache JMeter
										</a>
									</div>
									<div>
										<a href="https://www.neotys.com/neoload/overview" target="_blank">
											NeoLoad
										</a>
									</div>
									<div>
										<a href="https://loadninja.com/" target="_blank">
											LoadNinja
										</a>
									</div>
									<div>
										<a href="https://www.loadview-testing.com/" target="_blank">
											LoadView
										</a>
									</div>									
								</dd>
							</dl>
							<hr class="thin_90">
							<dl class="row">
								<dt class="col-2">
									Reference Implementation
								</dt>
								<dd class="col-10">
									...
								</dd>
							</dl>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</div>
</main>

<!-- scripts - base and vendor -->
<script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

<!-- scripts - page specific -->
<script src="../js/site-base.js"></script>

<script>
	$(function(){
		ReadySiteBase.init();
	});
</script>

</body>
</html>