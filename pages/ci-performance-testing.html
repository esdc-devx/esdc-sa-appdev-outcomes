<!DOCTYPE html>
<html lang="en">
<head>
	<title>Continuous Integration - Performance Testing</title>

	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	<link rel="stylesheet" href="../css/site-custom.css">
	<link rel="stylesheet" href="../css/pages/index.css">

	<link rel="icon" href="../favicon.png" type="image/png">
</head>
<body>

<nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
	<a class="navbar-brand pr-5" href="../index.html"><img class="logo" src="../images/logo_h.png"></img></a>

	<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navMain" aria-controls="navMain" aria-expanded="false" aria-label="Toggle navigation">
		<span class="navbar-toggler-icon"></span>
	</button>

	<div class="collapse navbar-collapse" id="navMain">
		<ul class="navbar-nav mr-auto">
			<li class="nav-item">
				<a class="nav-link" href="https://coggle.it/diagram/Xa8LPGI8UcxYqa_G/t/my-team-is-it-enabled-from-an-modern-perspective-red_cross/79ff70c4f2d83ea65ed42592541d7fa8494ab92bb690b77a269cea1ee0669153" target="_new">
					<span class="fa fa-share-alt"></span>
					Outcomes Map
				</a>
            </li>
			<li class="nav-item">
				<a class="nav-link" href="../index.html">
					<span class="fa fa-file-text-o"></span>
					Outcome Profiles
				</a>
			</li>
		</ul>
	</div>
</nav>

<main role="main" class="container-fluid">
<div class="box-main">
	<div class="container-fluid">
		<div class="row">
			<div class="col-9">
				<h1>Continuous Integration - Performance Testing</h1>
				<div class="v-spacer"></div>
				<ul class="nav nav-tabs" id="cpTabMain" role="tablist">
					<li class="nav-item">
						<a class="nav-link active" id="tab01" data-toggle="tab" href="#tbc01" role="tab" aria-controls="ctl01" aria-selected="true">
							Overview
						</a>
					</li>
					<li class="nav-item">
						<a class="nav-link" id="tab02" data-toggle="tab" href="#tbc02" role="tab" aria-controls="ctl02" aria-selected="false">
							Value
						</a>
					</li>
					<li class="nav-item">
						<a class="nav-link" id="tab03" data-toggle="tab" href="#tbc03" role="tab" aria-controls="ctl03" aria-selected="false">
							Best Practices
						</a>
					</li>
					<li class="nav-item">
						<a class="nav-link" id="tab04" data-toggle="tab" href="#tbc04" role="tab" aria-controls="ctl04" aria-selected="false">
							References
						</a>
					</li>
				</ul>
				<div class="tab-content" id="cpTabMainContent">
					<div class="tab-pane fade show active" id="tbc01" role="tabpanel" aria-labelledby="tab01">
						<div class="container-fluid">
							<dl class="row">
								<dt class="col-2">
									Definition
								</dt>
								<dd class="col-10">
									Performance Testing is a type of testing which ensures that a system will perform well under
									its expected workload. It serves to investigate, measure and address any system bottlenecks related to
									its speed (response time), scalability and stability.
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Desired State
								</dt>
								<dd class="col-10">
									Performance testing practices should be adopted by all dev teams and for all solutions, including custom and COTS.
									The execution of performance tests should be part of the build pipeline and be as fully automated.
								</dd>
							</dl>
							<hr	class="thin_90">
							<dl class="row">
								<dt class="col-2">
									Pathway Status
								</dt>
								<dd class="col-10">
									<span class="badge badge-warning">Partially Cleared</span>
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Status Notes
								</dt>
								<dd class="col-10">
									Tool selection needs to be expanded and supported.
								</dd>
							</dl>
							<hr class="thin_90">
							<dl class="row">
								<dt class="col-2">
									Realization Status
								</dt>
								<dd class="col-10">
									<span class="badge badge-danger">Low</span>
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Status Notes
								</dt>
								<dd class="col-10">
									<ul class="padding-none">
										<li>
											Dev team culture needs to be addressed.
										</li>
										<li>
											A standard set of guidance material should be made available.
										</li>
										<li>
											Reference implementations for the various programming languages and platforms should be made available.
										</li>
									</ul>
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Current SA Actions
								</dt>
								<dd class="col-10">
									<ul class="padding-none">
										<li>
											Support the introduction of performance testing to solution teams.
											<ul>
												<li>
													Will be working to raise awareness via DevCop and meeting with individual teams.
												</li>
												<li>
													Working on a standard set of guidance materials.
												</li>
											</ul>
										</li>
										<li>
											Management engagement to help promote the adoption within the teams.
										</li>
									</ul>
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Platforms
								</dt>
								<dd class="col-10">
									On-Premise, Cloud
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Environments
								</dt>
								<dd class="col-10">
									Non-PROD
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Solution Classes
								</dt>
								<dd class="col-10">
									All
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Programming Languages
								</dt>
								<dd class="col-10">
									All
								</dd>
							</dl>
						</div>
					</div>
					<div class="tab-pane fade" id="tbc02" role="tabpanel" aria-labelledby="tab02">
						<ul>
							<li>
								Helps determine if a system meets speed, scalability and stability requirements during various situations.
								<ul>
									<li>
										Users expect a system to respond as quickly as possible and when they donâ€™t, satisfaction decreases.
									</li>
									<li>
										Scalability is extremely important if you want more users to interact with the system.
									</li>
									<li>
										A system is expected to work at all times, when downtime occurs, users can lose confidence.
									</li>
								</ul>
							</li>
							<li>
								Helps uncover what system bottlenecks need to be improved before the product goes to production.
							</li>
							<li>
								Helps ensure that a system can run for a long period without deviations.
							</li>
							<li>
								Systems sent to production with good performance metrics are more likely to successfully meet their expected goals.
							</li>
						</ul>
					</div>
					<div class="tab-pane fade" id="tbc03" role="tabpanel" aria-labelledby="tab03">
						<dl>
							<dt>
								Consider the various types of performance testing
							</dt>
							<dd>
								<ul class="style-none padding-some">
									<li>
										<em>Load Testing</em>
										<br	/>
										Load testing checks the application's ability to perform under anticipated user loads. 
										The objective is to identify performance bottlenecks before the software application goes live.
									</li>
									<li>
										<em>Stress Testing</em>
										<br />
										This involves testing an application under extreme workloads to see how it handles high traffic or data
										processing. The objective is to identify the breaking point of an application.
									</li>
									<li>
										<em>Endurance Testing</em>
										<br />
										This type of testing is done to make sure the software can handle the expected load over a long period of time.
									</li>
									<li>
										<em>Spike Testing</em>
										<br />
										Spike testing verifies the software's reaction to sudden large spikes in the load generated by users.
									</li>
									<li>
										<em>Volume Testing</em>
										<br />
										The purpose of volume testing is to check a solution's performance when high levels of data volume come into play.
										The solution's data repositories are populated with large amounts of data and the is put through a series of test cases.
									</li>
									<li>
										<em>Scalability Testing</em>
										<br />
										The objective of scalability testing is to determine the software application's effectiveness in
										"scaling up" to support an increase in user load. It helps plan capacity addition to your software system.
									</li>
								</ul>
							</dd>
						</dl>
						<dl>
							<dt>
								Determine the important metrics for your solution
							</dt>
							<dd>
								<ul class="style-none padding-some">
									<li>
										<em>Average response time</em>
										<br />
										Average response time is a good metric to start gathering data on in your performance tests. Tracking this allows you
										see how your app fluctuates with more or less load. It gives you an idea of the average user experience, and it provides
										insight on regressions if something changes.
									</li>
									<li>
										<em>Peak response time</em>
										<br />
										Peak response time allows you to see the performance of slowest requests, generally by taking the 90th percentile of all
										response times. This creates a different view than an overall average. With peak response times, you can find more
										specific queries that may be problematic and know what the worst users are experiencing.
										Having a view into the slowest queries can make it easier to track down specific operations with latency, whereas the
										average response time is more general, and only gives you an idea of the entire system.
									</li>
									<li>
										<em>Error rates</em>
										<br />
										If errors occur, you'll want to know about them. Simply measuring a ratio of total requests by failed requests -- and
										failed requests can be anything that match a criteria you define -- gives you insight on what components start failing
										under a given load.
									</li>
									<li>
										<em>CPU utilization</em>
										<br />
										It's expected that CPU utilization will go up with more users, and go down with less, but do you know what happens when
										your CPU hits 100%? Being able to act preventatively under these conditions can help you build in redudancy and auto-scaling 
										systems to keep your app available.
									</li>
									<li>
										<em>Memory utilization</em>
										<br />
										Measuring memory usage during performance tests can capture the amount of memory used by the server when processing
										requests during different scenarios, like heavy load. This metric in particular is a good way to find memory leaks in
										your application, and it some cases can be coorelated with peak response times to determine slow database queries.
									</li>
								</ul>
							</dd>
						</dl>
						<dl>
							<dt>
								Test performance constantly and continuously
							</dt>
							<dd>
								Performance testing should be done as early and frequently in the process as as possible. 
								Some types of tests, like stress tests or other long-running test jobs, may be deferred to run near the end of a release
								cycle, but again: the earlier and more frequently tests can be run the better. Finding and resolving bottlenecks early
								in the development cycle saves developer time and hours.
							</dd>
						</dl>
						<dl>
							<dt>
								Run tests in a production-like environment
							</dt>
							<dd>
								Conducting performance testing in a test environment that is similar to the production environment is a performance
								testing best practice for a reason. The differences between the elements can significantly affect system performance. It
								may not be possible to conduct performance testing in the exact production environment, but try to match hardware
								components, operating system and settings, applications used on the system, and databases.

								All of the details here matter. A slightly different amount of memory or size CPU in a testing environment can skew
								results so much that they don't reflect how production will behave. Spending a little more time on this part to do the
								best you can is worth it in the long run.
							</dd>
						</dl>
						<dl>
							<dt>
								Scale your load testing
							</dt>
							<dd>
								Itâ€™s tempting to just run a test at the total load to find all the performance issues. Except for that kind of test
								tends to reveal so many performance issues that itâ€™s hard to focus on individual solutions. Starting at a lower load and
								scaling up incrementally may seem like an unnecessarily slow process, but it produces easier results that are more
								efficient to troubleshoot.
							</dd>
						</dl>
					</div>
					<div class="tab-pane fade" id="tbc04" role="tabpanel" aria-labelledby="tab04">
						<div class="container-fluid">
							<dl class="row">
								<dt class="col-2">
									Tool Examples
								</dt>
								<dd class="col-10">
									<div>
										<a href="https://jmeter.apache.org/" target="_blank">
											Apache JMeter
										</a>
									</div>
									<div>
										<a href="https://www.neotys.com/neoload/overview" target="_blank">
											NeoLoad
										</a>
									</div>
									<div>
										<a href="https://loadninja.com/" target="_blank">
											LoadNinja
										</a>
									</div>
								</dd>
							</dl>
							<hr class="thin_90">
							<dl class="row">
								<dt class="col-2">
									Reference Implementation -<br />.NET
								</dt>
								<dd class="col-10">
									...
								</dd>
							</dl>
							<dl class="row">
								<dt class="col-2">
									Reference Implementation -<br />Java
								</dt>
								<dd class="col-10">
									...
								</dd>
							</dl>
							<hr class="thin_90">
							<dl class="row">
								<dt class="col-2">
									Other Links
								</dt>
								<dd class="col-10">
									<a href="https://assertible.com/blog/web-service-performance-testing-tips-and-tools-for-getting-started">
										01
									</a>
									<br />
									<a href="https://stackify.com/ultimate-guide-performance-testing-and-software-testing/">
										02
									</a>
									<br />
									<a href="https://techbeacon.com/app-dev-testing/5-best-practices-realistic-performance-testing">
										03
									</a>
									<br />
									<a href="https://www.guru99.com/performance-testing.html">
										04
									</a>
									<br />
									<a href="https://en.wikipedia.org/wiki/Software_performance_testing">
										05
									</a>
									<br />
									<a href="https://www.oreilly.com/library/view/the-art-of/9781491900536/ch01.html">
										06
									</a>
								</dd>
							</dl>
						</div>
					</div>
				</div>
			</div>
		</div>
	</div>
</div>
</main>

<!-- scripts - base and vendor -->
<script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

</body>
</html>